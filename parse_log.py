import re
import csv
import os

def parse_log_file(log_file_path, output_csv_path):
    """
    è§£ææ·±åº¦å­¦ä¹ è®­ç»ƒæ—¥å¿—ï¼Œä¸ºæ¯ä¸ª Epoch ç”Ÿæˆä¸€è¡ŒåŒ…å«è®­ç»ƒå’Œè¯„ä¼°çš„å®Œæ•´æ•°æ®ã€‚
    
    æ–°è§„å®šåˆ—: 
    - Epoch, Avg_Training_Loss, Avg_Evaluation_Loss, 
    - Training_F1_1, Training_Precision_1, Training_Recall_1,
    - Evaluation_F1_1, Evaluation_Precision_1, Evaluation_Recall_1
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ—¥å¿—æ–‡ä»¶: {log_file_path}")

    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ—¥å¿—æ–‡ä»¶ '{log_file_path}'ã€‚")
        return
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚è¯¦ç»†ä¿¡æ¯: {e}")
        return

    # æ­£åˆ™è¡¨è¾¾å¼
    # æ•è· Epoch ç¼–å·ï¼Œä½œä¸ºå¯åŠ¨ä¸€ä¸ªæ–°æ•°æ®å®¹å™¨çš„ä¿¡å·
    epoch_start_pattern = re.compile(r"Is_training: True. \[(\d+),")
    train_loss_pattern = re.compile(r"Is_training: True.*G_loss: ([\d.]+)")
    eval_loss_pattern = re.compile(r"Is_training: False.*G_loss: ([\d.]+)")
    # æ­¤æ¨¡å¼ç”¨äºåŒ¹é…ä»»ä½•ä¸€ä¸ªæŒ‡æ ‡è¡Œ (è®­ç»ƒæˆ–è¯„ä¼°)
    metrics_pattern = re.compile(r"acc: .*?F1_1: ([\d.]+).*?precision_1: ([\d.]+).*?recall_1: ([\d.]+)")

    # --- æ ¸å¿ƒé€»è¾‘é‡æ„ ---
    all_epochs_data = []
    current_epoch_data = {}
    temp_train_losses = []
    temp_eval_losses = []

    for line in lines:
        epoch_match = epoch_start_pattern.search(line)
        # å½“æ‰¾åˆ°ä¸€ä¸ªè®­ç»ƒèµ·å§‹è¡Œæ—¶ï¼Œæˆ‘ä»¬åˆ¤æ–­æ˜¯å¦éœ€è¦å¼€å¯ä¸€ä¸ªæ–°çš„ Epoch è®°å½•
        if epoch_match:
            epoch_num = int(epoch_match.group(1))
            # å¦‚æœå½“å‰å®¹å™¨æ˜¯ç©ºçš„ï¼Œæˆ–è€…é‡åˆ°äº†ä¸€ä¸ªå…¨æ–°çš„epochç¼–å·
            if not current_epoch_data or current_epoch_data.get('Epoch') != epoch_num:
                current_epoch_data = {'Epoch': epoch_num}
                temp_train_losses = []
                temp_eval_losses = []

        # åªè¦å®¹å™¨è¢«åˆå§‹åŒ–ï¼Œå°±å¼€å§‹æ”¶é›†loss
        if current_epoch_data:
            train_loss_match = train_loss_pattern.search(line)
            if train_loss_match:
                temp_train_losses.append(float(train_loss_match.group(1)))

            eval_loss_match = eval_loss_pattern.search(line)
            if eval_loss_match:
                temp_eval_losses.append(float(eval_loss_match.group(1)))

            # å½“åŒ¹é…åˆ°ä»»ä½•ä¸€ä¸ªæŒ‡æ ‡è¡Œ (acc: ...)
            metrics_match = metrics_pattern.search(line)
            if metrics_match:
                f1_1 = float(metrics_match.group(1))
                precision_1 = float(metrics_match.group(2))
                recall_1 = float(metrics_match.group(3))

                # åˆ¤æ–­è¿™æ˜¯è®­ç»ƒæŒ‡æ ‡è¿˜æ˜¯è¯„ä¼°æŒ‡æ ‡
                if 'Training_F1_1' not in current_epoch_data:
                    # å¦‚æœå®¹å™¨é‡Œè¿˜æ²¡æœ‰è®­ç»ƒF1å€¼ï¼Œè¯´æ˜è¿™æ˜¯ç¬¬ä¸€ä¸ªæŒ‡æ ‡è¡Œï¼Œå³è®­ç»ƒæŒ‡æ ‡
                    current_epoch_data['Avg_Training_Loss'] = sum(temp_train_losses) / len(temp_train_losses) if temp_train_losses else None
                    current_epoch_data['Training_F1_1'] = f1_1
                    current_epoch_data['Training_Precision_1'] = precision_1
                    current_epoch_data['Training_Recall_1'] = recall_1
                else:
                    # å¦‚æœå®¹å™¨é‡Œå·²æœ‰è®­ç»ƒF1å€¼ï¼Œè¯´æ˜è¿™æ˜¯ç¬¬äºŒä¸ªæŒ‡æ ‡è¡Œï¼Œå³è¯„ä¼°æŒ‡æ ‡
                    current_epoch_data['Avg_Evaluation_Loss'] = sum(temp_eval_losses) / len(temp_eval_losses) if temp_eval_losses else None
                    current_epoch_data['Evaluation_F1_1'] = f1_1
                    current_epoch_data['Evaluation_Precision_1'] = precision_1
                    current_epoch_data['Evaluation_Recall_1'] = recall_1

                    # --- ä¸€ä¸ªEpochçš„æ•°æ®æ”¶é›†å®Œæ•´ï¼---
                    all_epochs_data.append(current_epoch_data)
                    # æ¸…ç©ºå®¹å™¨ï¼Œä¸ºä¸‹ä¸€ä¸ªepochåšå‡†å¤‡
                    current_epoch_data = {}

    # --- å†™å…¥CSVæ–‡ä»¶ ---
    if not all_epochs_data:
        print("ğŸ¤·â€ æœªèƒ½ä»æ—¥å¿—ä¸­æå–åˆ°ä»»ä½•å®Œæ•´çš„ Epoch æ•°æ®ã€‚")
        return

    # å®šä¹‰æ–°çš„è¡¨å¤´
    fieldnames = [
        'Epoch', 'Avg_Training_Loss', 'Avg_Evaluation_Loss', 
        'Training_F1_1', 'Training_Precision_1', 'Training_Recall_1',
        'Evaluation_F1_1', 'Evaluation_Precision_1', 'Evaluation_Recall_1'
    ]
    
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        # å†™å…¥æ—¶ï¼Œä½¿ç”¨ get(key, None) æ¥é¿å…å› æŸä¸ªå€¼ç¼ºå¤±å¯¼è‡´ç¨‹åºé”™è¯¯
        for row in all_epochs_data:
            writer.writerow({key: row.get(key) for key in fieldnames})

    print(f"âœ… æ•°æ®æå–å®Œæˆï¼å·²åˆå¹¶å¹¶ä¿å­˜åˆ°æ–‡ä»¶: {output_csv_path}")
    print(f"å…±å¤„ç†äº† {len(all_epochs_data)} ä¸ª Epoch çš„æ•°æ®ã€‚")


if __name__ == '__main__':
    input_log_file = '/data/gyf/newtest/ChangeFormer/checkpoint3/base_transformer_pos_s4_dd8_3/CD_base_transformer_pos_s4_dd8_LEVIR_b16_lr0.0001_adamw_train_test_201_linear_ce_multi_train_False_multi_infer_False_shuffle_AB_False_embed_dim_256/log.txt'
    output_csv_file = 'extracted_data_bit_withStyle.csv'
    
    if not os.path.exists(input_log_file):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶ '{input_log_file}'ã€‚")
    else:
        parse_log_file(input_log_file, output_csv_file)